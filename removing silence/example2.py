import numpy as np
import webrtcvad
import soundfile as sf
import librosa 

def frame_generator(y_int16, sr, frame_duration=30):
    """
    Generates audio frames of a specific duration (in ms) from a raw audio signal.
    
    The VAD (Voice Activity Detector) requires audio to be in fixed-size frames.
    """
    
    # Calculate the number of samples per frame.
    # e.g., 16000 Hz * 30 ms / 1000 = 480 samples
    n = int(sr * frame_duration / 1000)
    
    # Iterate over the audio signal in steps of frame size 'n'
    for offset in range(0, len(y_int16), n):
        # Extract the current frame
        frame = y_int16[offset:offset+n]
        
        # Check if the last frame is shorter than the required frame size
        if len(frame) < n:
            # If it is, pad it with zeros to make it the correct size.
            # webrtcvad requires all frames to be of the exact same length.
            pad = np.zeros(n - len(frame), dtype=np.int16)
            frame = np.concatenate([frame, pad])
            
        # 'yield' turns this function into a generator, returning one frame at a time
        yield frame

def vad_filter_safe(y, sr=16000, mode=2, frame_duration=30):
    """
    Filters an audio signal, returning only the segments detected as speech.
    
    y: The input audio signal (as floating-point numbers)
    sr: The sample rate (must be 8k, 16k, 32k, or 48k Hz for webrtcvad)
    mode: The aggressiveness of the VAD (0=least aggressive, 3=most aggressive)
    frame_duration: The frame size in ms (must be 10, 20, or 30 ms)
    """
    
    # --- 1. PREPARE AUDIO FOR VAD ---
    
    # Ensure audio data is within the valid range [-1.0, 1.0]
    y = np.clip(y, -1.0, 1.0)
    
    # Convert the floating-point audio signal to 16-bit PCM integer format.
    # This is the format required by webrtcvad.
    y_int16 = (y * 32767).astype(np.int16)
    
    # --- 2. INITIALIZE VAD ---
    
    # Create a VAD (Voice Activity Detector) object
    vad = webrtcvad.Vad(mode)
    
    # This list will hold all the frames that are classified as speech
    speech_frames = []
    
    # --- 3. PROCESS AUDIO IN FRAMES ---
    
    # Iterate through each frame generated by our frame_generator
    for i, frame in enumerate(frame_generator(y_int16, sr, frame_duration)):
        
        # Ask the VAD if the current frame is speech.
        # We must pass the audio as raw bytes (.tobytes())
        if vad.is_speech(frame.tobytes(), sr):
            # If it is speech, add it to our list
            speech_frames.append(frame)
            
    # --- 4. RECONSTRUCT AUDIO ---
    
    # If no speech frames were detected, return an empty audio array
    if not speech_frames:
        return np.array([], dtype=np.float32)
        
    # Concatenate all the detected speech frames back into a single audio signal
    # The result is still in 16-bit integer format
    speech = np.concatenate(speech_frames)
    
    # Convert the 16-bit integer signal back to the standard floating-point format
    speech = speech.astype(np.float32) / 32767.0
    
    # Return the final audio containing only speech
    return speech


y_denoised , sr = librosa.load('audio_files/trimmed_noise_reduction_output.wav', mono=True)
y_vad = vad_filter_safe(y_denoised, sr=16000, mode=2)
sf.write("removing silence/cleaned2.wav", y_vad, sr, subtype='PCM_16')
# 'subtype='PCM_16'' saves it as a standard 16-bit WAV file.
